\section{Introduction}
\label{introduction}
The light disperses into the surroundings, or it gets absorbed by the fragments present in the airspace. The total amount of fragments in the region is proportional to the scattering of light. This scattering reduces the clarity and colour in that region. Different particles and the environment they are in impact the scattering. Such as humidity and particles like sulphur cause a hazier environment. Most of these particles are present due to air pollution from industry. Some exist because of the natural world, such as windblown dust and forest fire. This haze, coupled with smoke, dust and low light condition, makes the haze removal task particularly difficult. This haze makes visibility extremely difficult for both humans and computer vision. e.g. the analysis from Wang et al. \cite{tourist}  concluded, the presence of haze can drastically effects tourist arrival. For better and more effective accuracy, vision algorithms expect clean, clear, and visible input. Since real-world places have haze environment tasks like object detection \cite{obj_det_cnn}, object segmentation \cite{motion_seg_cnn} becomes difficult due to a lack of information in captured images. To get the best out of these algorithms, we need to process real-world data to have the minimum possible haze and other artefacts. We can fix this via either pre-processing,  post-processing or both. Haze removal could be achievable in two ways physical and non-physical. The physical method analyses the atmospheric transmission that degrades the image and regenerates the image by inversing the existing transmission. The physical-based methods take the path of approximation and assumption about the prior condition. The non-physical-based techniques solve this problem through image enhancement. Popular approaches like Histogram equalization \cite{lai}, logarithmic transformation \cite{multi_sc}, various filters \cite{filters, pol_fil_sc, pol_filter2, prior_filt} in the frequency domain and Retinex theory \cite{retinex} are very robust and achieve remarkable results. These methods are not robust to the change and thus struggle with noise, different exposure settings and artefacts in the image. The core computer vision task needs clean and visible input to give a decent performance. Tasks like object detection, segmentation, and depth estimation deal with real-world data, these tasks struggle in performance when the images are hazy. We present a simple and miniature method to pre-process the input image, such that the performance of these tasks will improve.
\\
Recently, Deep learning-based methods perform exceptionally on the image enhancement task. These methods provide robust solutions to haze removal better than the quaint algorithm results. In this paper, we remove haze from both indoor and outdoor images. Our methods fix the different difficulties in the real world of hazy images like noise, challenging light and image artefacts. We constructed U-Net-like CNN \cite{u_net} with an encoder, bottleneck and decoder. Each of these consists of blocks of layers carefully designed for haze removal. These blocks consist of downsampling and upsampling layers with skip connections. We used a combination of loss functions to enhance different aspects of the images like the structural and perceptual quality of the image.
\\
The outline of the paper is summarized as follows. In this Section \ref{introduction}, we described haze and the difficulties faced in the haze removal process. Also, we explained the goal of the suggested method. In Section \ref{related_work}, we will discuss existing literature, the popular methods and their approach to solving the haze removal problem. The proposed technique and details about the model architecture and implementation are in Section \ref{proposed_method}. Training structure and comparison of our results with different methods are illustrated Section \ref{experiments}. Finally in Section \ref{conclusion}, we concluded the findings and summarised our approach of single image haze removal.